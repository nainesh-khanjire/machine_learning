{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict whether or not a patient has diabetes, based on certain diagnostic measurements\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "- The datasets consists of several medical predictor variables and one target variable, Outcome. \n",
    "- Predictor variables includes:\n",
    "    - the number of pregnancies the patient has had, \n",
    "    - their BMI, \n",
    "    - insulin level, \n",
    "    - age, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('../data/prima-indians-diabetes.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:,:8]\n",
    "Y = data[:,8:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train a classifer/Build diabetes prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a model using NB\n",
    "clf = GaussianNB()\n",
    "clf.fit(x_train, y_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_smoothing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-09\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Gaussian Naive Bayes (GaussianNB).\n",
       "\n",
       "Can perform online updates to model parameters via :meth:`partial_fit`.\n",
       "For details on algorithm used to update feature means and variance online,\n",
       "see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
       "\n",
       "    http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
       "\n",
       "Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "priors : array-like of shape (n_classes,)\n",
       "    Prior probabilities of the classes. If specified, the priors are not\n",
       "    adjusted according to the data.\n",
       "\n",
       "var_smoothing : float, default=1e-9\n",
       "    Portion of the largest variance of all features that is added to\n",
       "    variances for calculation stability.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "class_count_ : ndarray of shape (n_classes,)\n",
       "    number of training samples observed in each class.\n",
       "\n",
       "class_prior_ : ndarray of shape (n_classes,)\n",
       "    probability of each class.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    class labels known to the classifier.\n",
       "\n",
       "epsilon_ : float\n",
       "    absolute additive value to variances.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "sigma_ : ndarray of shape (n_classes, n_features)\n",
       "    Variance of each feature per class.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "       `sigma_` is deprecated in 1.0 and will be removed in 1.2.\n",
       "       Use `var_` instead.\n",
       "\n",
       "var_ : ndarray of shape (n_classes, n_features)\n",
       "    Variance of each feature per class.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "theta_ : ndarray of shape (n_classes, n_features)\n",
       "    mean of each feature per class.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
       "CategoricalNB : Naive Bayes classifier for categorical features.\n",
       "ComplementNB : Complement Naive Bayes classifier.\n",
       "MultinomialNB : Naive Bayes classifier for multinomial models.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
       ">>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
       ">>> from sklearn.naive_bayes import GaussianNB\n",
       ">>> clf = GaussianNB()\n",
       ">>> clf.fit(X, Y)\n",
       "GaussianNB()\n",
       ">>> print(clf.predict([[-0.8, -1]]))\n",
       "[1]\n",
       ">>> clf_pf = GaussianNB()\n",
       ">>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
       "GaussianNB()\n",
       ">>> print(clf_pf.predict([[-0.8, -1]]))\n",
       "[1]\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\pml1\\pml1\\lib\\site-packages\\sklearn\\naive_bayes.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### var_smoothing\n",
    "A Gaussian curve can serve as a \"low pass\" filter, allowing only the samples close to its mean to \"pass.\" In the context of Naive Bayes, assuming a Gaussian distribution is essentially giving more weights to the samples closer to the distribution mean. This might or might not be appropriate depending if what you want to predict follows a normal distribution.\n",
    "\n",
    "The variable, var_smoothing, artificially adds a user-defined value to the distribution's variance (whose default value is derived from the training data set). This essentially widens (or \"smooths\") the curve and accounts for more samples that are further away from the distribution mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Make predictions using test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on tets data\n",
    "predictions = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.7662337662337663\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy\n",
    "print('Accuracy Score: ', accuracy_score(predictions, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
