{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n",
    "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of data\n",
    "\n",
    "- The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
    "- This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions.\n",
    "- The dataset is highly <b>unbalanced</b>, the positive class (frauds) account for 0.172% of all transactions.\n",
    "- It contains only numerical input variables which are the result of a PCA transformation. \n",
    "- Due to confidentiality issues, the original features and more background information about the data is not provided. \n",
    "- Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. \n",
    "- Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset.\n",
    "- The feature 'Amount' is the transaction Amount.\n",
    "- Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "Source: [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional required packages\n",
    "# !pip install xgboost \n",
    "# !pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For scaling the features and train-test split\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "# For model buidling\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# For plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom module for prediction and model evalution\n",
    "#from utils import predict_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data file\n",
    "# this file is compressed in bzip2 format and index column is included in it\n",
    "df = pd.read_csv('../CC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Undersand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Time        V1        V2        V3        V4        V5  \\\n",
       "0           1   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321   \n",
       "1           2   0.0  1.191857  0.266151  0.166480  0.448154  0.060018   \n",
       "2           3   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198   \n",
       "3           4   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4           5   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193   \n",
       "\n",
       "         V6        V7        V8  ...       V21       V22       V23       V24  \\\n",
       "0  0.462388  0.239599  0.098698  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1 -0.082361 -0.078803  0.085102  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  1.800499  0.791461  0.247676  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  1.247203  0.237609  0.377436  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4  0.095921  0.592941 -0.270533  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    284807.000000\n",
       "mean         88.349619\n",
       "std         250.120109\n",
       "min           0.000000\n",
       "25%           5.600000\n",
       "50%          22.000000\n",
       "75%          77.165000\n",
       "max       25691.160000\n",
       "Name: Amount, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "Time          0\n",
       "V1            0\n",
       "V2            0\n",
       "V3            0\n",
       "V4            0\n",
       "V5            0\n",
       "V6            0\n",
       "V7            0\n",
       "V8            0\n",
       "V9            0\n",
       "V10           0\n",
       "V11           0\n",
       "V12           0\n",
       "V13           0\n",
       "V14           0\n",
       "V15           0\n",
       "V16           0\n",
       "V17           0\n",
       "V18           0\n",
       "V19           0\n",
       "V20           0\n",
       "V21           0\n",
       "V22           0\n",
       "V23           0\n",
       "V24           0\n",
       "V25           0\n",
       "V26           0\n",
       "V27           0\n",
       "V28           0\n",
       "Amount        0\n",
       "Class         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() # Check Null Values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8',\n",
       "       'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
       "       'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\n",
       "       'Amount', 'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 % of the dataset\n",
      "Frauds 0.17 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "# Check Distribution Of Label\n",
    "print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 284315 are normal transactions\n",
      "Frauds 492 are fraud\n"
     ]
    }
   ],
   "source": [
    "# The classes are heavily skewed. This is problem that needs to be solved. How?\n",
    "print('No Frauds', round(df['Class'].value_counts()[0],2), 'are normal transactions')\n",
    "print('Frauds', round(df['Class'].value_counts()[1],2), 'are fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class Distributions \\n (0: No Fraud || 1: Fraud)')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "\n",
    "sns.countplot(x='Class', data=df, palette=colors)\n",
    "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Notice how imbalanced is our original dataset! \n",
    "- Most of the transactions are non-fraud. \n",
    "- If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. \n",
    "- But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing - Scaling and Distribution\n",
    "- We will first scale the columns comprise of <b>Time</b> and <b>Amount </b>. \n",
    "- Time and amount should be scaled as the other columns. \n",
    "- On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n",
    "\n",
    "### What is a sub-Sample?\n",
    "In this scenario, our subsample will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n",
    "\n",
    "### Why do we create a sub-Sample?\n",
    "We saw that the original dataframe is heavily imbalanced! Using the original dataframe  will cause the following issues:\n",
    "<ul>\n",
    "<li><b>Overfitting: </b>Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs. </li>\n",
    "<li><b>Wrong Correlations:</b> Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scaling\n",
    "\n",
    "The **StandardScaler** assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1. \n",
    "\n",
    "$$\\frac{\\text{x}-\\text{mean}}{\\text{standard deviation}}$$\n",
    "\n",
    "The **MinMaxScaler** is the probably the most famous scaling algorithm, and follows the following formula for each feature. \n",
    "\n",
    "$$\\frac{\\text{x}-\\text{min}}{\\text{max}-\\text{min}}$$\n",
    "\n",
    "It essentially shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values). If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better. However, it is sensitive to outliers, so if there are outliers in the data, you might want to consider the Robust Scaler below.\n",
    "\n",
    "**Robust Scaler** scale features using statistics that are robust to outliers. The RobustScaler uses a similar method to the Min-Max scaler but it instead uses the interquartile range, rathar than the min-max, so that it is robust to outliers. \n",
    "\n",
    "$$\\frac{\\text{x}-\\text{Q1(x)}}{\\text{Q3(x)}-\\text{Q1(x)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Since most of our data has already been scaled, we will scale the columns that are not scaled (Amount and Time)\n",
    "# RobustScaler is less prone to outliers.\n",
    "rob_scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[149.62],\n",
       "       [  2.69],\n",
       "       [378.66],\n",
       "       ...,\n",
       "       [ 67.88],\n",
       "       [ 10.  ],\n",
       "       [217.  ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Amount'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1.783274\n",
       "1        -0.269825\n",
       "2         4.983721\n",
       "3         1.418291\n",
       "4         0.670579\n",
       "            ...   \n",
       "284802   -0.296653\n",
       "284803    0.038986\n",
       "284804    0.641096\n",
       "284805   -0.167680\n",
       "284806    2.724796\n",
       "Name: scaled_amount, Length: 284807, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.scaled_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    284807.000000\n",
       "mean          0.927124\n",
       "std           3.495006\n",
       "min          -0.307413\n",
       "25%          -0.229162\n",
       "50%           0.000000\n",
       "75%           0.770838\n",
       "max         358.683155\n",
       "Name: scaled_amount, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.scaled_amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(['Amount'], axis=1, inplace=True) # remove original time and Amount Columns from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>scaled_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>1.783274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.269825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>4.983721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>1.418291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.670579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Time        V1        V2        V3        V4        V5  \\\n",
       "0           1   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321   \n",
       "1           2   0.0  1.191857  0.266151  0.166480  0.448154  0.060018   \n",
       "2           3   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198   \n",
       "3           4   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4           5   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193   \n",
       "\n",
       "         V6        V7        V8  ...       V21       V22       V23       V24  \\\n",
       "0  0.462388  0.239599  0.098698  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1 -0.082361 -0.078803  0.085102  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  1.800499  0.791461  0.247676  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  1.247203  0.237609  0.377436  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4  0.095921  0.592941 -0.270533  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Class  scaled_amount  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053      0       1.783274  \n",
       "1  0.167170  0.125895 -0.008983  0.014724      0      -0.269825  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752      0       4.983721  \n",
       "3  0.647376 -0.221929  0.062723  0.061458      0       1.418291  \n",
       "4 -0.206010  0.502292  0.219422  0.215153      0       0.670579  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rearranging the columns\n",
    "scaled_amount = df['scaled_amount']\n",
    "\n",
    "df.drop(['scaled_amount'], axis=1, inplace=True)\n",
    "df.insert(0, 'scaled_amount', scaled_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.783274</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.269825</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.983721</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.418291</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670579</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount  Unnamed: 0  Time        V1        V2        V3        V4  \\\n",
       "0       1.783274           1   0.0 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1      -0.269825           2   0.0  1.191857  0.266151  0.166480  0.448154   \n",
       "2       4.983721           3   1.0 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3       1.418291           4   1.0 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4       0.670579           5   2.0 -1.158233  0.877737  1.548718  0.403034   \n",
       "\n",
       "         V5        V6        V7  ...       V20       V21       V22       V23  \\\n",
       "0 -0.338321  0.462388  0.239599  ...  0.251412 -0.018307  0.277838 -0.110474   \n",
       "1  0.060018 -0.082361 -0.078803  ... -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2 -0.503198  1.800499  0.791461  ...  0.524980  0.247998  0.771679  0.909412   \n",
       "3 -0.010309  1.247203  0.237609  ... -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4 -0.407193  0.095921  0.592941  ...  0.408542 -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Amount is Scaled!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**EXERCISE:** Scale the Time Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Splitting the DataFrame\n",
    "\n",
    "Before proceeding with any <b> Sampling technique</b> we have to separate the orginal dataframe.<br> \n",
    "<b> Why? for testing purposes, we want to test our models on the original testing set not on the testing set created by either of these techniques.</b><br> The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss = StratifiedShuffleSplit(n_splits=1,\n",
    "                            test_size=0.2,\n",
    "                            train_size=0.8,\n",
    "                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in ss.split(X, y):\n",
    "    train_df = df.iloc[train_index]\n",
    "    test_df = df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([265518, 180305,  42664, ...,  29062,  13766,  17677], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([263020,  11378, 147283, ..., 274532, 269819,  64170], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265518</th>\n",
       "      <td>-0.205128</td>\n",
       "      <td>265519</td>\n",
       "      <td>161919.0</td>\n",
       "      <td>1.946747</td>\n",
       "      <td>-0.752526</td>\n",
       "      <td>-1.355130</td>\n",
       "      <td>-0.661630</td>\n",
       "      <td>1.502822</td>\n",
       "      <td>4.024933</td>\n",
       "      <td>-1.479661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134435</td>\n",
       "      <td>0.076197</td>\n",
       "      <td>0.297537</td>\n",
       "      <td>0.307915</td>\n",
       "      <td>0.690980</td>\n",
       "      <td>-0.350316</td>\n",
       "      <td>-0.388907</td>\n",
       "      <td>0.077641</td>\n",
       "      <td>-0.032248</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180305</th>\n",
       "      <td>-0.265633</td>\n",
       "      <td>180306</td>\n",
       "      <td>124477.0</td>\n",
       "      <td>2.035149</td>\n",
       "      <td>-0.048880</td>\n",
       "      <td>-3.058693</td>\n",
       "      <td>0.247945</td>\n",
       "      <td>2.943487</td>\n",
       "      <td>3.298697</td>\n",
       "      <td>-0.002192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227279</td>\n",
       "      <td>0.038628</td>\n",
       "      <td>0.228197</td>\n",
       "      <td>0.035542</td>\n",
       "      <td>0.707090</td>\n",
       "      <td>0.512885</td>\n",
       "      <td>-0.471198</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>-0.069002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42664</th>\n",
       "      <td>2.139314</td>\n",
       "      <td>42665</td>\n",
       "      <td>41191.0</td>\n",
       "      <td>-0.991920</td>\n",
       "      <td>0.603193</td>\n",
       "      <td>0.711976</td>\n",
       "      <td>-0.992425</td>\n",
       "      <td>-0.825838</td>\n",
       "      <td>1.956261</td>\n",
       "      <td>-2.212603</td>\n",
       "      <td>...</td>\n",
       "      <td>1.280856</td>\n",
       "      <td>-2.798352</td>\n",
       "      <td>0.109526</td>\n",
       "      <td>-0.436530</td>\n",
       "      <td>-0.932803</td>\n",
       "      <td>0.826684</td>\n",
       "      <td>0.913773</td>\n",
       "      <td>0.038049</td>\n",
       "      <td>0.185340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198723</th>\n",
       "      <td>-0.222176</td>\n",
       "      <td>198724</td>\n",
       "      <td>132624.0</td>\n",
       "      <td>2.285718</td>\n",
       "      <td>-1.500239</td>\n",
       "      <td>-0.747565</td>\n",
       "      <td>-1.668119</td>\n",
       "      <td>-1.394143</td>\n",
       "      <td>-0.350339</td>\n",
       "      <td>-1.427984</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.490642</td>\n",
       "      <td>-0.139670</td>\n",
       "      <td>0.077013</td>\n",
       "      <td>0.208310</td>\n",
       "      <td>-0.538236</td>\n",
       "      <td>-0.278032</td>\n",
       "      <td>-0.162068</td>\n",
       "      <td>0.018045</td>\n",
       "      <td>-0.063005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82325</th>\n",
       "      <td>0.895689</td>\n",
       "      <td>82326</td>\n",
       "      <td>59359.0</td>\n",
       "      <td>-0.448747</td>\n",
       "      <td>-1.011440</td>\n",
       "      <td>0.115903</td>\n",
       "      <td>-3.454854</td>\n",
       "      <td>0.715771</td>\n",
       "      <td>-0.147490</td>\n",
       "      <td>0.504347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275297</td>\n",
       "      <td>-0.243245</td>\n",
       "      <td>-0.173298</td>\n",
       "      <td>-0.006692</td>\n",
       "      <td>-1.362383</td>\n",
       "      <td>-0.292234</td>\n",
       "      <td>-0.144622</td>\n",
       "      <td>-0.032580</td>\n",
       "      <td>-0.064194</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233802</th>\n",
       "      <td>-0.223713</td>\n",
       "      <td>233803</td>\n",
       "      <td>147710.0</td>\n",
       "      <td>1.993864</td>\n",
       "      <td>-0.516866</td>\n",
       "      <td>-0.620118</td>\n",
       "      <td>0.129845</td>\n",
       "      <td>-0.285128</td>\n",
       "      <td>0.395044</td>\n",
       "      <td>-0.822358</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174051</td>\n",
       "      <td>0.262526</td>\n",
       "      <td>0.884510</td>\n",
       "      <td>0.099141</td>\n",
       "      <td>0.275689</td>\n",
       "      <td>-0.195404</td>\n",
       "      <td>0.623598</td>\n",
       "      <td>-0.032455</td>\n",
       "      <td>-0.058552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85418</th>\n",
       "      <td>0.209460</td>\n",
       "      <td>85419</td>\n",
       "      <td>60764.0</td>\n",
       "      <td>-1.497933</td>\n",
       "      <td>0.657921</td>\n",
       "      <td>1.581568</td>\n",
       "      <td>-0.024286</td>\n",
       "      <td>0.584698</td>\n",
       "      <td>1.303031</td>\n",
       "      <td>0.609212</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225079</td>\n",
       "      <td>-0.072452</td>\n",
       "      <td>0.299172</td>\n",
       "      <td>0.110048</td>\n",
       "      <td>-0.615980</td>\n",
       "      <td>-0.425883</td>\n",
       "      <td>0.263968</td>\n",
       "      <td>-0.448445</td>\n",
       "      <td>0.045178</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29062</th>\n",
       "      <td>0.327255</td>\n",
       "      <td>29063</td>\n",
       "      <td>35301.0</td>\n",
       "      <td>1.069777</td>\n",
       "      <td>0.072105</td>\n",
       "      <td>0.496540</td>\n",
       "      <td>1.505318</td>\n",
       "      <td>-0.380277</td>\n",
       "      <td>-0.370243</td>\n",
       "      <td>0.100551</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149402</td>\n",
       "      <td>-0.061991</td>\n",
       "      <td>-0.044629</td>\n",
       "      <td>-0.050485</td>\n",
       "      <td>0.400171</td>\n",
       "      <td>0.593314</td>\n",
       "      <td>-0.335160</td>\n",
       "      <td>0.031014</td>\n",
       "      <td>0.024886</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13766</th>\n",
       "      <td>-0.294977</td>\n",
       "      <td>13767</td>\n",
       "      <td>24413.0</td>\n",
       "      <td>1.280465</td>\n",
       "      <td>0.300586</td>\n",
       "      <td>0.333044</td>\n",
       "      <td>0.512720</td>\n",
       "      <td>0.065052</td>\n",
       "      <td>-0.145844</td>\n",
       "      <td>-0.145519</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093783</td>\n",
       "      <td>-0.409934</td>\n",
       "      <td>-0.961351</td>\n",
       "      <td>0.033153</td>\n",
       "      <td>-0.560429</td>\n",
       "      <td>0.278428</td>\n",
       "      <td>0.089546</td>\n",
       "      <td>-0.059835</td>\n",
       "      <td>-0.005887</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17677</th>\n",
       "      <td>-0.209739</td>\n",
       "      <td>17678</td>\n",
       "      <td>28847.0</td>\n",
       "      <td>-0.598120</td>\n",
       "      <td>0.775041</td>\n",
       "      <td>1.823394</td>\n",
       "      <td>0.312991</td>\n",
       "      <td>-0.096171</td>\n",
       "      <td>-0.391452</td>\n",
       "      <td>0.499351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102259</td>\n",
       "      <td>0.060615</td>\n",
       "      <td>0.568083</td>\n",
       "      <td>-0.084001</td>\n",
       "      <td>0.685003</td>\n",
       "      <td>-0.245859</td>\n",
       "      <td>0.356638</td>\n",
       "      <td>0.378580</td>\n",
       "      <td>0.206366</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227845 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_amount  Unnamed: 0      Time        V1        V2        V3  \\\n",
       "265518      -0.205128      265519  161919.0  1.946747 -0.752526 -1.355130   \n",
       "180305      -0.265633      180306  124477.0  2.035149 -0.048880 -3.058693   \n",
       "42664        2.139314       42665   41191.0 -0.991920  0.603193  0.711976   \n",
       "198723      -0.222176      198724  132624.0  2.285718 -1.500239 -0.747565   \n",
       "82325        0.895689       82326   59359.0 -0.448747 -1.011440  0.115903   \n",
       "...               ...         ...       ...       ...       ...       ...   \n",
       "233802      -0.223713      233803  147710.0  1.993864 -0.516866 -0.620118   \n",
       "85418        0.209460       85419   60764.0 -1.497933  0.657921  1.581568   \n",
       "29062        0.327255       29063   35301.0  1.069777  0.072105  0.496540   \n",
       "13766       -0.294977       13767   24413.0  1.280465  0.300586  0.333044   \n",
       "17677       -0.209739       17678   28847.0 -0.598120  0.775041  1.823394   \n",
       "\n",
       "              V4        V5        V6        V7  ...       V20       V21  \\\n",
       "265518 -0.661630  1.502822  4.024933 -1.479661  ... -0.134435  0.076197   \n",
       "180305  0.247945  2.943487  3.298697 -0.002192  ... -0.227279  0.038628   \n",
       "42664  -0.992425 -0.825838  1.956261 -2.212603  ...  1.280856 -2.798352   \n",
       "198723 -1.668119 -1.394143 -0.350339 -1.427984  ... -0.490642 -0.139670   \n",
       "82325  -3.454854  0.715771 -0.147490  0.504347  ... -0.275297 -0.243245   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "233802  0.129845 -0.285128  0.395044 -0.822358  ... -0.174051  0.262526   \n",
       "85418  -0.024286  0.584698  1.303031  0.609212  ... -0.225079 -0.072452   \n",
       "29062   1.505318 -0.380277 -0.370243  0.100551  ... -0.149402 -0.061991   \n",
       "13766   0.512720  0.065052 -0.145844 -0.145519  ... -0.093783 -0.409934   \n",
       "17677   0.312991 -0.096171 -0.391452  0.499351  ...  0.102259  0.060615   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "265518  0.297537  0.307915  0.690980 -0.350316 -0.388907  0.077641 -0.032248   \n",
       "180305  0.228197  0.035542  0.707090  0.512885 -0.471198  0.002520 -0.069002   \n",
       "42664   0.109526 -0.436530 -0.932803  0.826684  0.913773  0.038049  0.185340   \n",
       "198723  0.077013  0.208310 -0.538236 -0.278032 -0.162068  0.018045 -0.063005   \n",
       "82325  -0.173298 -0.006692 -1.362383 -0.292234 -0.144622 -0.032580 -0.064194   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "233802  0.884510  0.099141  0.275689 -0.195404  0.623598 -0.032455 -0.058552   \n",
       "85418   0.299172  0.110048 -0.615980 -0.425883  0.263968 -0.448445  0.045178   \n",
       "29062  -0.044629 -0.050485  0.400171  0.593314 -0.335160  0.031014  0.024886   \n",
       "13766  -0.961351  0.033153 -0.560429  0.278428  0.089546 -0.059835 -0.005887   \n",
       "17677   0.568083 -0.084001  0.685003 -0.245859  0.356638  0.378580  0.206366   \n",
       "\n",
       "        Class  \n",
       "265518      0  \n",
       "180305      0  \n",
       "42664       0  \n",
       "198723      0  \n",
       "82325       0  \n",
       "...       ...  \n",
       "233802      0  \n",
       "85418       0  \n",
       "29062       0  \n",
       "13766       0  \n",
       "17677       0  \n",
       "\n",
       "[227845 rows x 32 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227845, 32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56962, 32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributions: \n",
      "\n",
      "Train Set\n",
      "0    227451\n",
      "1       394\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Test Set\n",
      "0    56864\n",
      "1       98\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "Percentage:\n",
      "\n",
      "Train Set\n",
      "0    99.827075\n",
      "1     0.172925\n",
      "Name: Class, dtype: float64\n",
      "\n",
      "Test Set\n",
      "0    99.827955\n",
      "1     0.172045\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Distributions: \\n')\n",
    "print(\"Train Set\")\n",
    "print(train_df.Class.value_counts())\n",
    "print(\"\\nTest Set\")\n",
    "print(test_df.Class.value_counts())\n",
    "print(\"\\nPercentage:\")\n",
    "print(\"\\nTrain Set\")\n",
    "print((train_df.Class.value_counts()/ len(train_df))*100)\n",
    "print(\"\\nTest Set\")\n",
    "print((test_df.Class.value_counts()/ len(test_df))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Under-Sampling:\n",
    "\n",
    "Implement *\"Random Under Sampling\"* which basically consists of removing data in order to have a more <b> balanced dataset </b> and thus avoiding our models to overfitting.\n",
    "\n",
    "**Steps:**\n",
    "<ul>\n",
    "<li>The first thing we have to do is determine how <b>imbalanced</b> is our class (use \"value_counts()\" on the class column to determine the amount for each label)  </li>\n",
    "<li>Once we determine how many instances are considered <b>fraud transactions </b> (Fraud = \"1\") , we should bring the <b>non-fraud transactions</b> to the same amount as fraud transactions (assuming we want a 50/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.  </li>\n",
    "<li> After implementing this technique, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. Then the next step we will implement is to <b>shuffle the data</b> to see if our models can maintain a certain accuracy everytime we run this script.</li>\n",
    "</ul>\n",
    "\n",
    "**Note:** The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of <b>information loss</b> (randomly picking 394 non-fraud transaction  from 2,27,451 non-fraud transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets shuffle the data before creating the subsamples\n",
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13341</th>\n",
       "      <td>1.592818</td>\n",
       "      <td>13342</td>\n",
       "      <td>23535.0</td>\n",
       "      <td>0.919391</td>\n",
       "      <td>-0.358937</td>\n",
       "      <td>0.163997</td>\n",
       "      <td>0.195361</td>\n",
       "      <td>-0.118717</td>\n",
       "      <td>0.170339</td>\n",
       "      <td>-0.082684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095272</td>\n",
       "      <td>0.010106</td>\n",
       "      <td>0.038430</td>\n",
       "      <td>-0.090397</td>\n",
       "      <td>-0.254736</td>\n",
       "      <td>0.139015</td>\n",
       "      <td>1.027888</td>\n",
       "      <td>-0.113085</td>\n",
       "      <td>-0.000992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144461</th>\n",
       "      <td>1.858311</td>\n",
       "      <td>144462</td>\n",
       "      <td>86142.0</td>\n",
       "      <td>0.976459</td>\n",
       "      <td>0.124219</td>\n",
       "      <td>0.278652</td>\n",
       "      <td>2.263769</td>\n",
       "      <td>0.272891</td>\n",
       "      <td>0.486166</td>\n",
       "      <td>0.198848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298372</td>\n",
       "      <td>-0.195539</td>\n",
       "      <td>-0.907044</td>\n",
       "      <td>-0.167065</td>\n",
       "      <td>-1.016435</td>\n",
       "      <td>0.357505</td>\n",
       "      <td>-0.192691</td>\n",
       "      <td>-0.015306</td>\n",
       "      <td>0.048689</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92414</th>\n",
       "      <td>-0.269825</td>\n",
       "      <td>92415</td>\n",
       "      <td>63922.0</td>\n",
       "      <td>0.043455</td>\n",
       "      <td>0.985073</td>\n",
       "      <td>0.520996</td>\n",
       "      <td>0.713213</td>\n",
       "      <td>0.635452</td>\n",
       "      <td>-0.449527</td>\n",
       "      <td>1.137316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206712</td>\n",
       "      <td>0.078524</td>\n",
       "      <td>0.799022</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.201554</td>\n",
       "      <td>-0.887311</td>\n",
       "      <td>-0.463935</td>\n",
       "      <td>0.222951</td>\n",
       "      <td>-0.168614</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87322</th>\n",
       "      <td>0.499406</td>\n",
       "      <td>87323</td>\n",
       "      <td>61634.0</td>\n",
       "      <td>1.103299</td>\n",
       "      <td>-0.067842</td>\n",
       "      <td>0.158375</td>\n",
       "      <td>0.686546</td>\n",
       "      <td>-0.551064</td>\n",
       "      <td>-1.053346</td>\n",
       "      <td>0.183233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170515</td>\n",
       "      <td>0.010422</td>\n",
       "      <td>-0.121390</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>0.610278</td>\n",
       "      <td>0.317656</td>\n",
       "      <td>0.470297</td>\n",
       "      <td>-0.065593</td>\n",
       "      <td>0.011498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7538</th>\n",
       "      <td>-0.146720</td>\n",
       "      <td>7539</td>\n",
       "      <td>10311.0</td>\n",
       "      <td>0.364827</td>\n",
       "      <td>0.184528</td>\n",
       "      <td>1.767462</td>\n",
       "      <td>0.534418</td>\n",
       "      <td>-0.413319</td>\n",
       "      <td>-0.083546</td>\n",
       "      <td>-0.163918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122299</td>\n",
       "      <td>-0.230930</td>\n",
       "      <td>-0.122954</td>\n",
       "      <td>0.089885</td>\n",
       "      <td>-0.030256</td>\n",
       "      <td>-0.674837</td>\n",
       "      <td>0.133205</td>\n",
       "      <td>-0.200304</td>\n",
       "      <td>-0.291796</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95292</th>\n",
       "      <td>-0.307413</td>\n",
       "      <td>95293</td>\n",
       "      <td>65242.0</td>\n",
       "      <td>-0.631701</td>\n",
       "      <td>1.049422</td>\n",
       "      <td>1.452194</td>\n",
       "      <td>2.576940</td>\n",
       "      <td>0.534141</td>\n",
       "      <td>-0.231687</td>\n",
       "      <td>0.583737</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029957</td>\n",
       "      <td>0.191249</td>\n",
       "      <td>0.602310</td>\n",
       "      <td>0.204187</td>\n",
       "      <td>0.418955</td>\n",
       "      <td>-0.307745</td>\n",
       "      <td>0.082757</td>\n",
       "      <td>-0.193404</td>\n",
       "      <td>0.147585</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14180</th>\n",
       "      <td>1.376790</td>\n",
       "      <td>14181</td>\n",
       "      <td>25213.0</td>\n",
       "      <td>-0.493876</td>\n",
       "      <td>0.684138</td>\n",
       "      <td>1.844956</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>-0.620155</td>\n",
       "      <td>-0.248437</td>\n",
       "      <td>1.257436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067721</td>\n",
       "      <td>-0.006462</td>\n",
       "      <td>0.332250</td>\n",
       "      <td>-0.181242</td>\n",
       "      <td>0.718239</td>\n",
       "      <td>0.201906</td>\n",
       "      <td>-0.386271</td>\n",
       "      <td>-0.162314</td>\n",
       "      <td>-0.190629</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19421</th>\n",
       "      <td>-0.232236</td>\n",
       "      <td>19422</td>\n",
       "      <td>30264.0</td>\n",
       "      <td>-0.314496</td>\n",
       "      <td>1.205763</td>\n",
       "      <td>1.164643</td>\n",
       "      <td>-0.033338</td>\n",
       "      <td>0.294177</td>\n",
       "      <td>-0.743782</td>\n",
       "      <td>0.794121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202264</td>\n",
       "      <td>-0.273375</td>\n",
       "      <td>-0.660949</td>\n",
       "      <td>-0.069127</td>\n",
       "      <td>0.005213</td>\n",
       "      <td>-0.073130</td>\n",
       "      <td>0.090236</td>\n",
       "      <td>0.252157</td>\n",
       "      <td>0.097157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198102</th>\n",
       "      <td>0.661217</td>\n",
       "      <td>198103</td>\n",
       "      <td>132333.0</td>\n",
       "      <td>1.908849</td>\n",
       "      <td>-0.040217</td>\n",
       "      <td>-1.670517</td>\n",
       "      <td>0.474229</td>\n",
       "      <td>0.157995</td>\n",
       "      <td>-1.093715</td>\n",
       "      <td>0.269766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010637</td>\n",
       "      <td>-0.200780</td>\n",
       "      <td>-0.378622</td>\n",
       "      <td>0.107054</td>\n",
       "      <td>-0.249158</td>\n",
       "      <td>-0.039482</td>\n",
       "      <td>-0.086242</td>\n",
       "      <td>-0.007004</td>\n",
       "      <td>-0.010389</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226705</th>\n",
       "      <td>2.964857</td>\n",
       "      <td>226706</td>\n",
       "      <td>144763.0</td>\n",
       "      <td>-1.841173</td>\n",
       "      <td>-0.796002</td>\n",
       "      <td>1.886521</td>\n",
       "      <td>0.579820</td>\n",
       "      <td>0.398037</td>\n",
       "      <td>-0.365021</td>\n",
       "      <td>0.532746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637447</td>\n",
       "      <td>0.453427</td>\n",
       "      <td>0.537048</td>\n",
       "      <td>0.319734</td>\n",
       "      <td>1.078898</td>\n",
       "      <td>0.785480</td>\n",
       "      <td>-0.494538</td>\n",
       "      <td>-0.043501</td>\n",
       "      <td>0.117334</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227845 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_amount  Unnamed: 0      Time        V1        V2        V3  \\\n",
       "13341        1.592818       13342   23535.0  0.919391 -0.358937  0.163997   \n",
       "144461       1.858311      144462   86142.0  0.976459  0.124219  0.278652   \n",
       "92414       -0.269825       92415   63922.0  0.043455  0.985073  0.520996   \n",
       "87322        0.499406       87323   61634.0  1.103299 -0.067842  0.158375   \n",
       "7538        -0.146720        7539   10311.0  0.364827  0.184528  1.767462   \n",
       "...               ...         ...       ...       ...       ...       ...   \n",
       "95292       -0.307413       95293   65242.0 -0.631701  1.049422  1.452194   \n",
       "14180        1.376790       14181   25213.0 -0.493876  0.684138  1.844956   \n",
       "19421       -0.232236       19422   30264.0 -0.314496  1.205763  1.164643   \n",
       "198102       0.661217      198103  132333.0  1.908849 -0.040217 -1.670517   \n",
       "226705       2.964857      226706  144763.0 -1.841173 -0.796002  1.886521   \n",
       "\n",
       "              V4        V5        V6        V7  ...       V20       V21  \\\n",
       "13341   0.195361 -0.118717  0.170339 -0.082684  ...  0.095272  0.010106   \n",
       "144461  2.263769  0.272891  0.486166  0.198848  ...  0.298372 -0.195539   \n",
       "92414   0.713213  0.635452 -0.449527  1.137316  ...  0.206712  0.078524   \n",
       "87322   0.686546 -0.551064 -1.053346  0.183233  ... -0.170515  0.010422   \n",
       "7538    0.534418 -0.413319 -0.083546 -0.163918  ... -0.122299 -0.230930   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "95292   2.576940  0.534141 -0.231687  0.583737  ... -0.029957  0.191249   \n",
       "14180   0.816149 -0.620155 -0.248437  1.257436  ... -0.067721 -0.006462   \n",
       "19421  -0.033338  0.294177 -0.743782  0.794121  ...  0.202264 -0.273375   \n",
       "198102  0.474229  0.157995 -1.093715  0.269766  ...  0.010637 -0.200780   \n",
       "226705  0.579820  0.398037 -0.365021  0.532746  ...  0.637447  0.453427   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "13341   0.038430 -0.090397 -0.254736  0.139015  1.027888 -0.113085 -0.000992   \n",
       "144461 -0.907044 -0.167065 -1.016435  0.357505 -0.192691 -0.015306  0.048689   \n",
       "92414   0.799022  0.000950  0.201554 -0.887311 -0.463935  0.222951 -0.168614   \n",
       "87322  -0.121390  0.017268  0.610278  0.317656  0.470297 -0.065593  0.011498   \n",
       "7538   -0.122954  0.089885 -0.030256 -0.674837  0.133205 -0.200304 -0.291796   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "95292   0.602310  0.204187  0.418955 -0.307745  0.082757 -0.193404  0.147585   \n",
       "14180   0.332250 -0.181242  0.718239  0.201906 -0.386271 -0.162314 -0.190629   \n",
       "19421  -0.660949 -0.069127  0.005213 -0.073130  0.090236  0.252157  0.097157   \n",
       "198102 -0.378622  0.107054 -0.249158 -0.039482 -0.086242 -0.007004 -0.010389   \n",
       "226705  0.537048  0.319734  1.078898  0.785480 -0.494538 -0.043501  0.117334   \n",
       "\n",
       "        Class  \n",
       "13341       0  \n",
       "144461      0  \n",
       "92414       0  \n",
       "87322       0  \n",
       "7538        0  \n",
       "...       ...  \n",
       "95292       0  \n",
       "14180       0  \n",
       "19421       0  \n",
       "198102      0  \n",
       "226705      0  \n",
       "\n",
       "[227845 rows x 32 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# amount of fraud classes 394 rows\n",
    "fraud_df = train_df.loc[train_df['Class'] == 1]\n",
    "non_fraud_df = train_df.loc[train_df['Class'] == 0][:394]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As fraud_df and non_fraud_df are concatenated, Shuffle dataframe rows to mix the rows\n",
    "df2 = normal_distributed_df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(788, 32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153758</th>\n",
       "      <td>0.384266</td>\n",
       "      <td>153759</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>-1.408633</td>\n",
       "      <td>-1.624698</td>\n",
       "      <td>2.547742</td>\n",
       "      <td>0.385672</td>\n",
       "      <td>0.502790</td>\n",
       "      <td>0.507195</td>\n",
       "      <td>-1.744431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209033</td>\n",
       "      <td>0.217641</td>\n",
       "      <td>0.758247</td>\n",
       "      <td>0.281254</td>\n",
       "      <td>0.736608</td>\n",
       "      <td>-0.741402</td>\n",
       "      <td>0.255350</td>\n",
       "      <td>0.141944</td>\n",
       "      <td>0.228167</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154633</th>\n",
       "      <td>0.000559</td>\n",
       "      <td>154634</td>\n",
       "      <td>102318.0</td>\n",
       "      <td>-1.020632</td>\n",
       "      <td>1.496959</td>\n",
       "      <td>-4.490937</td>\n",
       "      <td>1.836727</td>\n",
       "      <td>0.627318</td>\n",
       "      <td>-2.735569</td>\n",
       "      <td>-1.546274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443489</td>\n",
       "      <td>0.773631</td>\n",
       "      <td>0.860618</td>\n",
       "      <td>-0.304666</td>\n",
       "      <td>-0.155500</td>\n",
       "      <td>0.412166</td>\n",
       "      <td>-0.220080</td>\n",
       "      <td>0.392338</td>\n",
       "      <td>-0.020089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15166</th>\n",
       "      <td>1.089779</td>\n",
       "      <td>15167</td>\n",
       "      <td>26523.0</td>\n",
       "      <td>-18.474868</td>\n",
       "      <td>11.586381</td>\n",
       "      <td>-21.402917</td>\n",
       "      <td>6.038515</td>\n",
       "      <td>-14.451158</td>\n",
       "      <td>-4.146524</td>\n",
       "      <td>-14.856124</td>\n",
       "      <td>...</td>\n",
       "      <td>1.577924</td>\n",
       "      <td>1.741136</td>\n",
       "      <td>-1.251138</td>\n",
       "      <td>-0.396219</td>\n",
       "      <td>0.095706</td>\n",
       "      <td>1.322751</td>\n",
       "      <td>-0.217955</td>\n",
       "      <td>1.628793</td>\n",
       "      <td>0.482248</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57470</th>\n",
       "      <td>-0.296793</td>\n",
       "      <td>57471</td>\n",
       "      <td>47923.0</td>\n",
       "      <td>0.364377</td>\n",
       "      <td>1.443523</td>\n",
       "      <td>-2.220907</td>\n",
       "      <td>2.036985</td>\n",
       "      <td>-1.237055</td>\n",
       "      <td>-1.728161</td>\n",
       "      <td>-2.058582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310980</td>\n",
       "      <td>0.402730</td>\n",
       "      <td>-0.132129</td>\n",
       "      <td>-0.032977</td>\n",
       "      <td>0.460861</td>\n",
       "      <td>0.560404</td>\n",
       "      <td>0.409366</td>\n",
       "      <td>0.539668</td>\n",
       "      <td>0.296918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45203</th>\n",
       "      <td>-0.293440</td>\n",
       "      <td>45204</td>\n",
       "      <td>42247.0</td>\n",
       "      <td>-2.524012</td>\n",
       "      <td>2.098152</td>\n",
       "      <td>-4.946075</td>\n",
       "      <td>6.456588</td>\n",
       "      <td>3.173921</td>\n",
       "      <td>-3.058806</td>\n",
       "      <td>-0.184710</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162797</td>\n",
       "      <td>0.027935</td>\n",
       "      <td>0.220366</td>\n",
       "      <td>0.976348</td>\n",
       "      <td>-0.290539</td>\n",
       "      <td>1.161002</td>\n",
       "      <td>0.663954</td>\n",
       "      <td>0.456023</td>\n",
       "      <td>-0.405682</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_amount  Unnamed: 0      Time         V1         V2         V3  \\\n",
       "153758       0.384266      153759  100000.0  -1.408633  -1.624698   2.547742   \n",
       "154633       0.000559      154634  102318.0  -1.020632   1.496959  -4.490937   \n",
       "15166        1.089779       15167   26523.0 -18.474868  11.586381 -21.402917   \n",
       "57470       -0.296793       57471   47923.0   0.364377   1.443523  -2.220907   \n",
       "45203       -0.293440       45204   42247.0  -2.524012   2.098152  -4.946075   \n",
       "\n",
       "              V4         V5        V6         V7  ...       V20       V21  \\\n",
       "153758  0.385672   0.502790  0.507195  -1.744431  ...  0.209033  0.217641   \n",
       "154633  1.836727   0.627318 -2.735569  -1.546274  ...  0.443489  0.773631   \n",
       "15166   6.038515 -14.451158 -4.146524 -14.856124  ...  1.577924  1.741136   \n",
       "57470   2.036985  -1.237055 -1.728161  -2.058582  ...  0.310980  0.402730   \n",
       "45203   6.456588   3.173921 -3.058806  -0.184710  ... -0.162797  0.027935   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "153758  0.758247  0.281254  0.736608 -0.741402  0.255350  0.141944  0.228167   \n",
       "154633  0.860618 -0.304666 -0.155500  0.412166 -0.220080  0.392338 -0.020089   \n",
       "15166  -1.251138 -0.396219  0.095706  1.322751 -0.217955  1.628793  0.482248   \n",
       "57470  -0.132129 -0.032977  0.460861  0.560404  0.409366  0.539668  0.296918   \n",
       "45203   0.220366  0.976348 -0.290539  1.161002  0.663954  0.456023 -0.405682   \n",
       "\n",
       "        Class  \n",
       "153758      0  \n",
       "154633      1  \n",
       "15166       1  \n",
       "57470       1  \n",
       "45203       1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Equally Distributing \n",
    "<a id=\"correlating\"></a>\n",
    "Now that we have our dataframe correctly balanced, we can go further with our <b>analysis</b> and <b>data preprocessing</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the Classes in the subsample dataset\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Distribution of the Classes in the subsample dataset')\n",
    "print(df2['Class'].value_counts()/len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "sns.countplot(x='Class', data=df2, palette=colors)\n",
    "plt.title('Equally Distributed Classes', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training the ML Model for Fraud Detection(Classification) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create X_train, X_test, y_train, y_test for ease of use\n",
    "X_train = df2.drop('Class', axis=1)\n",
    "y_train = df2['Class']\n",
    "\n",
    "X_test = test_df.drop('Class', axis=1)\n",
    "y_test = test_df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=100, criterion=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_features=&#x27;sqrt&#x27;,\n",
       "                       random_state=909181773)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_features=&#x27;sqrt&#x27;,\n",
       "                       random_state=909181773)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_features='sqrt',\n",
       "                       random_state=909181773)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree in the Forest\n",
    "rf_clf.estimators_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluation Metrics\n",
    "The Given the class imbalance ratio, Confusion matrix and accuracy is not meaningful\n",
    "for unbalanced classification. A robust evaluation is required to measure the\n",
    "performance of a fraud detection model.\n",
    "\n",
    "**1. False Positives:**\n",
    "A false positive is an outcome where the model incorrectly predicts the positive class.\n",
    "<br>**2. False Negatives:**\n",
    "A false negative is an outcome where the model incorrectly predicts the negative class.\n",
    "<br>**3. Precision:**\n",
    "Precision talks about how precise/accurate the model is i.e. out of those predicted positives, how many of them are actual positive. Precision is a good measure to determine, when the costs of False Positives is high. For instance, here, a false positive means that a transaction is that is non- fraudulent has been identified as fraudulent. This can happen if the precision is not high for the fraud detection model.\n",
    "<br>**4. Recall:**\n",
    "Recall calculates how many of the Actual Positives our model captures through labeling it as Positive (True Positive). If a fraudulent transaction is predicted as non-fraudulent (Predicted Negative), the consequence can be very bad for the bank.\n",
    "<br>**5. F1 Score:**\n",
    "F1 Score is used to seek a balance between Precision and Recall.\n",
    "<br>**6. Mathews Correlation Coefficient:**\n",
    "The coefficient takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between −1 and +1.<br> \n",
    "A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation.<br>\n",
    "The Matthews correlation coefficient is more informative than F1 score and\n",
    "accuracy in evaluating binary classification problems, because it takes into\n",
    "account the balance ratios of the four confusion matrix categories (true\n",
    "positives, true negatives, false positives, and false negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_and_evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rf_res \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_and_evaluate\u001b[49m(rf_clf, X_test, y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_and_evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "rf_res = predict_and_evaluate(rf_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Importances\n",
    "\n",
    "In order to quantify the usefulness of all the variables in the entire random forest, we can look at the relative importances of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(rf_clf.feature_importances_, index=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4), dpi=100)\n",
    "feature_importances.plot.bar()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Mean Decrease in Impurity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise\n",
    "Train all models again by considering only important features (e.g. top 10 or 20) and evaluate the model and observe the difference in the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gbm_clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gbm_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gbm_res = predict_and_evaluate(gbm_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_res = predict_and_evaluate(xgb_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparing the metrics for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data=[rf_res, gbm_res, xgb_res], \n",
    "             columns=('Algorithm','False Positives', \n",
    "                      'False Negatives', 'Precision', \n",
    "                      'Recall', 'F1 Score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
